- **目标**：在 20 周内系统完成从基础→离线→实时→治理→面试的学习闭环，产出 2 个可展示项目与可抗打的面试题库，目标岗位为**大数据工程师（成都，15K）**。
- **总时间**：20 周；每周 15–20 小时（工作日 2h，周末 4–6h）。
- **权重分配**：基础 20%｜离线数仓 25%｜实时计算 25%｜数据治理 15%｜面试 15%。
- **硬核必修**：SQL 优化、Spark 核心与 Shuffle、Flink 核心与状态管理/Exactly-once、Kafka 事务与分区、维度建模。
- **两大项目产出**：
  1. **离线电商数仓项目**（Hadoop+Hive+Spark+Airflow）
     . **实时交易风控/实时指标项目**（Kafka+Flink+（Doris/Hudi））

## 二、分（Week-by-Week 每周计划）

> 说明：每周标注**目标**、**重点**、**时长建议**、**交付物**。如你每周可投入 15 小时，按此执行即可；若 20 小时，可把练习/加分项加满。

### **阶段 1 基础夯实（Week 1–4）｜总 60h｜权重 20%**

**Week 1：Linux + Shell 入门（15h）**

- 目标：掌握日常大数据开发常用命令与脚本能力
- 重点：文件/权限/进程/网络（`ls`,`grep`,`awk`,`sed`,`chmod`,`ps`,`netstat`），bash 基础
- 交付物：一份 20+ 常用命令备忘单 & 2 个自动化脚本（日志切割、增量备份）
- **Linux & Shell**：《鸟哥的Linux私房菜-基础学习篇》或尚硅谷/黑马程序员的免费Linux基础视频（B站）

**Week 2：SQL 进阶 I（15h）**

- 目标：写出多表 JOIN、窗口函数的分析 SQL
- 重点：窗口函数、复杂 JOIN、子查询、CTE
- 交付物：完成 30 道中高难 SQL 题（含解析）+ Explain 使用笔记
- **SQL进阶**：LeetCode数据库题库（中等难度）、牛客网SQL实战。**《SQL必知必会》和《高性能MySQL》**​ 中关于索引和查询优化的章节是经典。

**Week 3：分布式与 HDFS/Hive 概念（15h）**

- 目标：理解 Hadoop 生态与数据存储布局
- 重点：CAP、HDFS 架构、MapReduce 概念、Hive 基础 DDL/DML
- 交付物：本地伪分布式环境安装笔记 + Hive 表建模示例（分区/分桶/压缩）
- **Hadoop/Spark/Flink**：**尚硅谷、黑马程序员**的免费系列课程（B站可搜到），体系完整，适合入门。同时，**官方文档**是解决高级问题和深入原理的终极宝典

**Week 4：Java/Python 基础与并发入门（15h）**

- 目标：能写稳健的 ETL/工具类；理解线程池
- 重点：Java 集合、IO、线程池；或 Python pandas 基础
- 交付物：实现一个小 ETL 脚本（CSV→聚合→输出），附单元测试

---

### **阶段 2 离线数仓与 Spark（Week 5–8）｜总 75h｜权重 25%**

**Week 5：Hive 表设计与分区策略（20h）**

- 目标：能为电商日志/交易数据设计合理的 ODS/DWD 表
- 重点：外/内部表、分区/分桶、文件格式（Parquet/ORC）、压缩（Snappy）
- 交付物：ODS+DWD 表结构 SQL（含分区策略说明）

**Week 6：Spark 核心与 Spark SQL（20h）**

- 目标：掌握 RDD→DataFrame→Spark SQL 的使用
- 重点：宽窄依赖、Action/Transformation、Catalyst 优化器
- 交付物：3 个 Spark 作业：日志清洗、明细宽表聚合、TopN 统计

**Week 7：Shuffle 与性能优化（20h）**

- 目标：理解并优化 Shuffle 性能
- 重点：Shuffle 原理、倾斜治理（盐值/两阶段聚合）、并行度设置、缓存与广播
- 交付物：一份「倾斜问题排查与治理手册」+ 优化前后对比报告

**Week 8：Airflow 调度与离线项目闭环（15h）**

- 目标：把离线链路串起来并实现重试/监控
- 重点：DAG、依赖、失败重试、定时调度、任务幂等
- 交付物：**离线电商数仓项目 v1**（HDFS→Hive→Spark→ADS），带 DAG 图与 README
  离线项目中加入数据质量校验

---

### **阶段 3 实时计算（Week 9–12）｜总 75h｜权重 25%**

**Week 9：Kafka 基础与可靠投递（15h）**

- 目标：能稳定生产/消费与分区均衡
- 重点：生产者 ACK/重试/幂等、分区策略、副本、Consumer Group、事务
- 交付物：生产消费压测脚本 + 消费位点管理说明

**Week 10：Flink 核心 API（20h）**

- 目标：使用 DataStream/Table API 实现实时清洗/聚合
- 重点：Source/Sink、Keyed State、算子链、并行度
- 交付物：实时清洗任务（维表关联前置版）+ Table API 指标聚合 demo
- **每周抽出30-60分钟，持续温习和整理面试题**

**Week 11：Watermark/窗口/状态/Exactly-once（20h）**

- 目标：应对乱序与端到端一致性
- 重点：Watermark 策略、滚动/滑动/会话窗口、Checkpoint/Savepoint、事务 Sink
- 交付物：乱序场景用例与指标准确性验收报告

**Week 12：Flink CDC & 实时项目闭环（20h）**

- 目标：打通 MySQL→Kafka→Flink→Doris/Hudi 实时链路
- 重点：CDC 断点续传、Upsert、维表 Join、异步 IO
- 交付物：**实时风控/指标项目 v1**（含拓扑图、指标口径文档）考虑血缘记录

---

### **阶段 4 数据治理与平台化（Week 13–16）｜总 45h｜权重 15%**

**Week 13：数据质量（10h）**

- 目标：建立质量规则与告警
- 重点：唯一性/完整性/及时性、抽样校验、阈值与告警
- 交付物：质量校验规则集 + 质量日报模板

**Week 14：元数据与血缘（12h）**

- 目标：记录表/字段血缘并可视化
- 重点：Atlas/Amundsen 概念、血缘采集、标签与权限
- 交付物：关键表的血缘图（离线+实时）与字段字典

**Week 15：指标平台与统一口径（11h）**

- 目标：沉淀通用指标、统一口径
- 重点：原子/派生/复合指标、口径变更管理、指标复用
- 交付物：指标仓清单（含口径定义、计算口径、数据域）

**Week 16：成本优化与存储治理（12h）**

- 目标：降低计算/存储成本
- 重点：冷热分层、分区修剪、Compaction、数据保留策略
- 交付物：存储成本评估表 + 优化执行清单

---

### **阶段 5 面试准备（Week 17–20）｜总 45h｜权重 15%**

**Week 17：高频问 + SQL 优化（12h）**

- 目标：覆盖 80% 高频面试点
- 重点：SQL 执行计划、索引与倾斜、常见场景题
- 交付物：个人 SQL 题库（≥50 题）+ 优化套路卡

**Week 18：Spark/Flink 原理与系统设计（11h）**

- 目标：能清晰讲原理与取舍
- 重点：Spark Shuffle/Catalyst/存储计算分离；Flink 状态/Checkpoint/一致性
- 交付物：两份系统设计方案（离线、实时各一份），带容量评估

**Week 19：项目深挖与STAR法（11h）**

- 目标：形成能“打动面试官”的项目叙述
- 重点：问题—方案—效果—量化指标
- 交付物：项目总结 PPT（离线+实时），每个项目 5–8 页

**Week 20：模拟面试与简历定稿（11h）**

- 目标：查漏补缺、强化表达
- 重点：自我介绍、项目追问、故障定位题、开放题
- 交付物：两次模拟面试记录（问题&改进）+ 简历定稿（1 页）

---

## 三、回顾与执行建议（总）

- **执行节奏**：每周固定 2 次阶段回顾（周三/周日 30 分钟），更新清单与下周计划。
- **产出导向**：每个阶段至少有一个**可演示的成果**（代码仓库、DAG/拓扑图、指标口径文档、优化报告）。
- **面试倒推**：从第 10 周开始，每周加入 1 小时面试题温习；第 17 周起进入冲刺模式。
- **优先级提醒**：若时间紧，优先保证——SQL 优化、Spark Shuffle、Flink 状态&Exactly-once、Kafka 事务与分区、维度建模与口径统一。

### 

**主题**：SQL & Linux & Python/Java & 分布式理论  
**里程碑**：

- 能写**复杂SQL**（窗口函数、CTE、索引与执行计划）；
- 熟练Linux常用命令（文件、权限、进程、网络、shell）；
- 掌握数据结构/并发基础（Java）或pandas/pySpark（Python）。  
  **练手**：把常见业务报表需求用SQL实现：用户留存、漏斗、转化、分群。  
  **产出**：一套SQL题库+脚本库。

---

### 第5–8周（离线数仓与Spark生态）

**主题**：HDFS/Hive、Spark（RDD/DataFrame/Spark SQL）、数据分层（ODS/DWD/DWS/ADS）、维度建模（星型/雪花）、调度（Airflow/Apache DolphinScheduler）  
**里程碑**：

- 设计并实现**电商/日志**主题的离线数仓，**日增量+全量**；
- 优化：分区、分桶、压缩（Parquet/ORC）、倾斜处理；
- 用**Airflow**编排依赖与重试，产出数据字典。  
  **练手项目**：日志采集→清洗→DWD→DWS→ADS，产出UV/GMV/复购率周报。  
  **产出**：架构图、任务DAG、数据字典与指标口径说明。

---

### 第9–12周（实时计算与流批一体）

**主题**：Kafka、Flink（DataStream/Table/SQL）、状态管理与Checkpoint、Exactly-once、Watermark与乱序、CDC（Debezium/Flink CDC）  
**里程碑**：

- 构建：**Kafka→Flink**实时ETL与聚合，支持**多主题多消费者**；
- 处理乱序、迟到、状态膨胀；
- **Flink SQL**做实时指标与标签。  
  **练手项目**：实时交易风控或实时热度榜单；接入**Flink CDC**做MySQL→Kafka→Doris/Hudi。  
  **产出**：延迟与吞吐监控（Prometheus+Grafana），报警策略与回压处理说明。

---

### 第13–16周（数据治理与平台化）

**主题**：数据质量（空值/重复/约束/一致性）、血缘与元数据（Atlas/Amundsen）、权限与审计、成本优化（存储与计算）、指标平台（**统一口径**）  
**里程碑**：

- 建**数据质量规则**与拦截；
- 上线**元数据/血缘**；
- 推出**指标平台**与复用规范；
- 性价比优化：冷热分层、Compaction、资源队列。  
  **练手项目**：为前述离线+实时链路加治理与可观测性，输出SLA与RTO/RPO文档。  
  **产出**：质量报表、血缘图、SLA与应急预案。

---

### 第17–20周（专项强化与面试准备）

**主题**：性能调优、故障注入演练、系统设计、面试题库整理、模拟面试  
**里程碑**：

- Spark/Flink **真实性能分析**（Task/Shuffle/GC/背压）；
- 设计题：企业级实时数仓、指标中心、流批一体与湖仓一体（Hudi/Iceberg/Delta）；
- 完成≥2次**模拟面试**（基础+项目+系统设计+问答）。  
  **产出**：项目总结PPT（10–15页）、简历（STAR）、面经与题库。

> **加速/减速**：已有SQL/Hive经验可把基础段压缩到2周；若零基础，建议把基础段延长至6–8周，并先做纯离线项目再上实时。

## 二、推荐资源（真能用）

### 课程/文档

- **Kafka/Flink 官方文档**：体系最全、概念精准，项目落地必读。
  - Apache Kafka docs（生产消费、事务、Exactly-once） [[zhipin.com]](https://www.zhipin.com/zhaopin/247ca8ebf724fb1e03Bz3tm4FA~~/)
  - Apache Flink docs（Data/State/Watermark/CEP/Flink SQL） [[zhipin.com]](https://www.zhipin.com/zhaopin/247ca8ebf724fb1e03Bz3tm4FA~~/)
- **Airflow** 官方 + “User Guide”
- **Spark**：《Spark: The Definitive Guide》（DataFrame/Spark SQL实战）
- **数据建模**：**《大数据之路：阿里巴巴大数据实践》** Ralph Kimball《The Data Warehouse Toolkit》（维度建模经典）
- **数据治理**：Apache Atlas/Amundsen官方文档（元数据/血缘）

**项目实战**：在完成自有项目后，可去**天池、Kaggle**等平台寻找公开数据集，尝试解决新问题，以丰富简历。
